\chapter{Introduction}

Over the last forty years quantum computers have moved from a theoretically interesting concept to exciting experimental and engineering proposals. We now have many suggested applications which quantum computers can solve faster, from physical and chemical simulations to aid in the development of medicines and fertilizers \cite{berry2019}, to machine learning so that companies such as Netflix can offer better recommendations to their customers \cite{kerenidis2017}.

Perhaps even more exciting is the fact that these computers have gone from simply being a theoretical exercise to actual physical devices that we can program right here and now. Companies such as IBM and Rigetti allow people to access and program real-world quantum computers from the comfort of their web browser \cite{ibm, rigetti}. And they even now exist at a scale where they can solve some problems faster than our best classical computers, as was recently demonstrated by Google \cite{arute2019}.

But despite the phenomenal progress made, there is still a ways to go before quantum computers are \emph{useful}. For example, the problem solved by Google's quantum computer is a highly contrived problem with no known applications. Thus, while this result is a significant milestone, there is still work that needs to be done in order to demonstrate the full power of quantum computing.

This thesis focuses on the next step on the roadmap towards truly beneficial quantum computers: Trying to find a useful problem which can be solved faster by a quantum computer in the near future. We shall do this by focusing on two directions: One is application-focused, where we look for real-world problems which quantum computers can solve faster; the other is architecture-focused, where we look at what a near-term quantum computing architecture needs to achieve to outpace classical computers. One can think of these two approaches as ``top-down'' and ``bottom-up'' respectively.

In the remainder of this chapter we shall provide more detail on all of these points. First in Section \ref{sec:eventual-power}, we shall discuss some of the best-known applications that sufficiently large quantum computers will be able to solve faster. Next in Section \ref{sec:near-term-power}, we will talk about our aims for what we want a near-term quantum computer to achieve. In Section \ref{sec:contributions}, we explain our contributions to this milestone and summarise the following chapters. Finally, we mention some notation that is used throughout this thesis in Section \ref{sec:notation}.

\section{The (eventual) power of quantum computing}
\label{sec:eventual-power}

The theory of quantum computing has shown the potential for speedups in a large number of problems, some of which we shall detail below. For further information on these problems, we direct the reader to \cite{montanaro2016}.

The original proposed application of quantum computation is the simulating of physical systems. Indeed, this application was proposed as early as 1981, when Richard Feynman spoke about needing quantum mechanics to simulate nature during a keynote he gave at The California Institute of Technology \cite{feynman1982}. It then took 15 more years for a general quantum algorithm to be able to simulate any quantum physical system, proven by Lloyd in 1996 \cite{lloyd1996}. In the decades since, other quantum simulation algorithms have made significant improvements \cite{berry2015, berry2015stoc, low2017}.

Following Feynman's original proposal, the first exponential speedup was given by Deutsch and Jozsa in 1992 \cite{deutsch1992}, who gave a query problem which required an exponential number of classical queries but only one quantum query to solve exactly. However, this was for a contrived problem with no applications. The first quantum algorithm with applications to the real world was by Shor in 1994 \cite{shor1994}, who gave an algorithm for factoring integers and computing discrete logarithms in polynomial time. These problems are believed to be classically hard to solve, and efficient computation would lead to being able to break popular encryption schemes\cite{katz2007}.

Many other applications have also appeared in the following years. In 1997, Grover \cite{grover96} showed how quantum computers can be used to find marked items quadratically faster than performing a na\"ive search. And in 2009, Harrow, Hassidim and Lloyd (HHL) provided the first quantum algorithm for solving linear systems of equations \cite{harrow2009}. This lead to many applications, from Support Vector Machines \cite{rebentrost2014} to Recommendation Systems \cite{kerenidis2017}, in the field of Quantum Machine Learning \cite{wittek2014, schuld2014, adcock2015, biamonte2017}. This has also led to the more recent direction of quantum-inspired classical machine learning, where various ideas from quantum algorithms have been applied to classical algorithms via sampling rows and columns of matrices with low rank \cite{tang2019, tang2018, gilyen2018, arrazola2019, chia2019, jethwani2019}.

\section{What quantum computers can achieve in the near term}
\label{sec:near-term-power}

However, several of these problems seem to be far off from offering a benefit in the near term. For example, the largest number factored via Shor's algorithm is 21 \cite{vandersypen2001, lanyon2007, lu2007, politi2009, lucero2012, martinlopez2012}, far smaller than numbers considered cryptographically relevant. Furthermore, implementations of Shor's algorithm so far have used classical compilation methods which take advantage of the already known solution to simplify the quantum algorithm, even though when taken to extremes this can simplify Shor's algorithm to a single unbiased coin toss \cite{smolin2013}. Other quantum computing techniques designed for near-term architectures, such as adiabatic \cite{xu2012, dattani2014, dridi2017, li2017} or variational \cite{anschuetz2018, crane2019} algorithms, have been able to factor numbers as large as 291,311 and 1,099,551,473,989, respectively. But it is not clear how the performance of these schemes scales \cite{aaronson2019mottebailey}, and these integers are still smaller than what is used in modern encryption schemes. For reference, classical computers have managed to factor RSA-240, which is 240 digits (795 bits) long \cite{boudot2019}. Likewise, experimental implementations of HHL have so far only solved linear systems consisting of 2 variables \cite{cai2013, pan2014, barz2014}, and related algorithms such as the Quantum Support Vector Machine have only been implemented to successfully distinguish the written characters of ``6'' and ``9'' \cite{li2015}.

While it is interesting to consider problems which quantum computers will eventually be able to solve faster, there is still a need to motivate building these devices now. Hence, the fundamental question of this thesis is the following: What can a quantum computer do in the near future that a classical computer cannot?

As of 2019 there is arguably a very simple answer to this question: Random Circuit Sampling (RCS). This problem, first specified by Boixo et al.~\cite{boixo2018}, is to sample from the output of a random quantum circuit whose depth is linear in the number of qubits. This problem was proven to be classically hard under certain assumptions \cite{hangleiter2018, bouland2018} -- explained in more detail in Chapter \ref{chp:preliminary_bs}) -- but a quantum computer would be able to solve it by simply implementing the random circuit and making a measurement. And most excitingly, an instance of the problem has now been solved on a quantum computer developed by Google which we believe to be hard to solve classically in a reasonable amount of time \cite{arute2019}.

However, there are a number of limitations with this model. First, there are no known applications of RCS. Our interest in the problem is not because of its practical uses, but instead simply because we have strong evidence for the problem being classically hard. Second, Google's device is not especially good at performing RCS. The metric used to verify correctness, a variant of fidelity, is estimated for Google's device as roughly 0.1-0.2\%. Google defends this low fidelity by simply reducing their estimate of a classical computer's performance by a factor of 1,000, again emphasising classical hardness over usefulness. And third, there is still a lot of uncertainty and debate as to how quickly a classical computer can perform this problem. Google's paper estimated it would take a state of the art supercomputer 10,000 years \cite{arute2019}, whereas a recent preprint by IBM estimates the best current supercomputer could perform the same computation in roughly 2.5 days if used solely for this computation \cite{pednault2019}. Even further, a report by Morimae, Takeuchi and Tani suggests that classical computers could simulate Google's device even faster, as the fidelity is so low that even sampling from the uniform distribution could be sufficiently precise \cite{morimae2019google}. Similar claims have also been made in a recent preprint by Zhou, Stoudenmire and Waintal \cite{zhou2020}, who have claimed to simulate Google's device on a single core computer in a matter of hours. This was achieved by using Matrix Product States, a classical simulation technique where the limiting factor is the multi-qubit fidelity, rather than the number of qubits or circuit depth.

It is with these limitations in mind that we broadly define what we mean when we refer to a near-term quantum speedup. Ideally, we want to find a problem that meets three requirements. The first is that this problem can be applied to real-world problems, rather than only being interesting because of its hardness. The second is that a quantum computer can be constructed in the next few years that can solve this problem in a reasonable amount of time. And the third is that this problem is infeasible to solve on a classical computer, or will at least take significantly longer than the time required for a quantum computer. We have purposefully left these requirements broad rather than giving specific details. This is in order to allow room for a variety of problems, which might satisfy some of the requirements more than others.

One direction that quickly comes to mind that fits these requirements well is the field of Noisy Intermediate-Scale Quantum (NISQ) devices \cite{preskill2018}. When Preskill coined the term at a Keynote Address, a variety of examples were given from quantum simulations to quantum semi-definite programming, as examples of practical problems with known quantum speedups that could be experimentally feasible in the near future. NISQ is often used to refer to two quantum systems in particular: The Variational Quantum Eigensolver (VQE) \cite{peruzzo2014}, which estimates eigenstates of some target Hamiltonian $H$ via a combination of quantum state preparation, Hamiltonian evolution and measurement; and the Quantum Adiabatic Optimisation Algorithm \cite{farhi2014}, which uses a combination of alternating unitary rotations and classical optimisation to solve various combinatorial optimisation problems, and was recently proven to offer the same dynamics as universal quantum computation, for correctly chosen Hamiltonian times \cite{lloyd2018, morales2019}. Both of these offer potential for meeting our requirements for a quantum speedup, and are strong directions of future research. However, we will not focus on these ideas for the content of this thesis, instead prioritising other research topics.

\section{Our contributions}
\label{sec:contributions}

In this thesis, we make some progress towards finding this quantum speedup. We shall do this in two directions, laid out below.

\subsection{An application-focused perspective}

Our first direction is an application-focused, or ``top-down'', approach. This is through taking a problem that cannot be efficiently solved on a classical computer and seeing if a quantum computer can solve it faster.

The problem we focus on in particular is the Travelling Salesman Problem (TSP), where the aim is to find the fastest way of visiting every city on a map. This problem is well-known in Computer Science and Mathematics, and has been considered by academics for over 150 years \cite{schrijver2005}. As an NP-Hard problem, finding an efficient classical solution would solve the long-standing P vs NP problem, bringing with it a million dollar prize as well as many other significant consequences to the world around us \cite{cmipvsnp, aaronson2006}. Being NP-Hard also means that it is highly unlikely for an efficient quantum algorithm to be found, but it does not rule out the possibility that quantum computers can solve this problem faster than classical computers. Our first contribution is to demonstrate this positively, by proving polynomial speedups for the Travelling Salesman Problem in the special case where the graph is of bounded degree. This is achieved by showing how classical algorithms for solving the bounded-degree TSP \cite{eppstein2007, xiao2016degree3, xiao2016degree4} can be sped up using a quantum algorithm for the general family of classical algorithms known as backtracking algorithms \cite{montanaro2015}.

This approach is directed at satisfying our first and third requirements, that the problem is applicable to the real-world and is difficult to solve classically. We shall not explicitly cover the second requirement, that a near-term quantum computer can solve this efficiently, as part of our contributions, but we will make some comments about other progress that has been made towards this requirement and what can be done to improve things further \cite{dunjko2018, ge2019}.

\subsection{An architecture-focused perspective}

Our second direction is an architecture-focused, or ``bottom-up'' approach. Here we will take a near-term architecture for a quantum computer and consider how much more powerful it could be than classical computation.

In particular, we shall consider Boson Sampling, a non-universal model of quantum computation consisting of sampling indistinguishable photons input into a random linear optical interferometer \cite{aaronson2010report, aaronson2011}. It was proven by Aaronson and Arkhipov that this architecture is classically infeasible to simulate for sufficiently many photons and a large enough interferometer, assuming certain conjectures related to matrix permanents and computational complexity hold. What was significant about Boson Sampling in particular was that this was a very simple model, with no error correction required, and simply used methods and components in linear optics which were already well-known and understood. This led to significant interest, including many experimental advances \cite{broome2013, spring2013, tillmann2013, crespi2013, carolan2015, wang2017, paesani2018, wang2019}, as well as architectural variants \cite{aaronson2013, lund2014, hamilton2017}.

However, there are practical issues with Boson Sampling experiments which were left unaddressed in the original result. In particular, how photon distinguishability and loss affect the classical complexity of Boson Sampling. These issues have been studied in recent years, with some exciting results \cite{garciapatron2017, renema2018, renema2018loss, oszmaniec2018, brod2019}. In this thesis, we make two contributions to this side. First, we describe a link between Boson Sampling and representation theory, via the Quantum Schur Transform \cite{bacon2004, harrow2005, bacon2007}. In doing so, we are able to model distinguishability and loss as different forms of decoherence in a specially-structured quantum circuit. Second, we take this model and apply it to a specific form of distinguishability and loss noted in \cite{renema2018, renema2018loss}. We then adapt the best asymptotic classical simulation algorithm for Boson Sampling \cite{clifford2017} to take advantage of distinguishability and loss, and in doing so produce a classical algorithm which can better simulate Boson Sampling in near-term experiments under these imperfections.

This direction is aimed more at satisfying our second and third requirements of a quantum speedup, in that Boson Sampling is a problem which can be easily done by a quantum computer, such as a Boson Sampling device, but is classically infeasible. This direction less satisfies the first option, in that there are not any known applications for the standard form of Boson Sampling. However, interferometers used for Boson Sampling have also been used for other applications, such as simulating the vibronic spectra of molecules \cite{sparrow2018}, and Boson Sampling variants have also found applications in both simulations and combinatorics \cite{huh2015, bradler2018, schuld2019}.

\subsection{Structure of this thesis}

The remainder of this thesis is laid out as follows. In Part \ref{prt:application}, we shall focus on the applications perspective. First, in Chapter \ref{chp:prelim-q-c} we explain the significance of NP-Hard problems, general classical algorithms for these problems and their corresponding quantum speedups. In Chapter \ref{chp:tsp} we give an explicit example of a speedup for an NP-Hard problem, by giving polynomial speedups for the Travelling Salesman Problem for bounded degree graphs. Next, in Part \ref{prt:architecture} we switch to the architecture perspective. We start in Chapter \ref{chp:preliminary_bs} by explaining the need for near-term quantum architectures, introducing the theory behind Boson Sampling and its imperfections, and explaining representation theory of the Symmetric and Unitary groups. In Chapter \ref{chp:noisy_circuit}, we go into detail about the link between Boson Sampling and representation theory, and in doing so explain how distinguishability and loss can be modelled as decoherence in a quantum circuit of particular structure. We then in Chapter \ref{chp:classical_sim} use this model to develop a classical simulation algorithm for Boson Sampling under a particular form of these imperfections, and show that this algorithm will be a more effective simulator for near-term devices with 30-50 photons. We conclude with some final points and open questions in Chapter \ref{chp:conclusion}.

\subsection{Previous publications and collaborations}

Some of the contents of this thesis has been published previously, or completed in collaboration with others. All work in these chapters was led and written by myself except where noted. Permission for reuse of content, where applicable, has been acquired. {\tt arXiv:arxiv\_id} is used to refer to manuscripts which have been uploaded to the arXiv e-print repository -- \url{https://arxiv.org/} -- where preprints of these manuscripts are freely accessible.

\begin{itemize}
\item Parts of Chapter \ref{chp:prelim-q-c} and the main results of Chapter \ref{chp:tsp} are joint work with Noah Linden and Ashley Montanaro, and has been published as ``Quantum speedup of the traveling-salesman problem for bounded-degree graphs'', \href{https://link.aps.org/doi/10.1103/PhysRevA.95.032323}{\textit{Physical Review A} \textbf{95}, 032323 (2017)}, copyright American Physics Society. A preprint of this article is freely available at {\tt \href{https://arxiv.org/abs/1612.06203}{arXiv:1612.06203}}. Note that this work was completed and published under my former name.

\item Parts of Chapter \ref{chp:preliminary_bs} and the main results of Chapter \ref{chp:noisy_circuit} are joint work with Peter S.\ Turner, and has been published as ``Quantum simulation of partially distinguishable boson sampling'', \href{https://link.aps.org/doi/10.1103/PhysRevA.97.062329}{\textit{Physical Review A} \textbf{97}, 062329 (2018)}, copyright American Physics Society. A preprint of this article is freely available at {\tt \href{https://arxiv.org/abs/1803.03657}{arXiv:1803.03657}}.

\item Parts of Chapter \ref{chp:preliminary_bs} and the main results of Chapter \ref{chp:classical_sim} is joint work with Ra\'{u}l Garc\'{i}a-Patr\'{o}n, Jelmer J. Renema and Peter S. Turner, and has been published as as ``Classically simulating near-term partially-distinguishable and lossy boson sampling'', \href{https://iopscience.iop.org/article/10.1088/2058-9565/ab5555}{\textit{Quantum Science and Technology} \textbf{5}, 015001 (2020)}, copyright Institute of Physics. A preprint of this article is freely available at {\tt \href{https://arxiv.org/abs/1907.00022}{arXiv:1907.00022}}.
\end{itemize}

\section{A brief note on complexity theory}
\label{sec:notation}

Both Parts \ref{prt:application} and \ref{prt:architecture} make extensive use of concepts from complexity theory. We summarise these concepts below for the benefit of the reader.

Throughout this thesis we will make extensive use of Big-O (and related) notation to simplify the expression of runtimes. For functions $f, g\colon \mathbb{R}_+\rightarrow \mathbb{R}_+$, we say that

\begin{equation}
f(n) \in O(g(n)) \textrm{ if } \exists n_0, c \in \mathbb{R} \textrm{ such that } f(n) \leq cg(n) \forall n \geq n_0,
\end{equation}

\noindent and

\begin{equation}
f(n) \in \Omega(g(n)) \textrm{ if } \exists n_0, c \in \mathbb{R} \textrm{ such that } f(n) \geq cg(n) \forall n \geq n_0.
\end{equation}

Intuitively, this notation allows us to simplify expressions by ignoring constant factors and focusing only on the most significant bottlenecks in a runtime. We direct the curious reader to \cite{clrs} for further details. We shall also use $O^*$ to suppress factors which are polynomial in $n$, the input size, $\poly(n)$ to denote terms which are polynomial in $n$, and $\polylog(n)$ for terms which are polynomial in $\log n$.

A variety of complexity classes are discussed in this thesis too. P and NP will be explained in more detail in Section \ref{sec:p-vs-np}, but informally: P is the class of problems which a classical computer can solve in polynomial time, and NP the class of problems which a classical computer can verify a solution for in polynomial time. We shall also mention BPP, the class of problems which can be solved probabilistically on a classical computer in polynomial time with probability greater than 2/3; BQP, the class of problems which can be solved on a quantum computer with probability greater than 2/3; and $\sharpp$, the class of problems around counting the number of solutions to a problem in NP. The last two classes we mention are NP-Hard and $\sharpp$-Hard, which are the classes of problems which are at least as hard as any problem in NP and $\sharpp$, respectively. Further details on these and many other complexity classes can be found at the Complexity Zoo \cite{complexityzoo}.
