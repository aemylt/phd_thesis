\chapter{Introduction}

\section{The (eventual) power of quantum computing}

\section{What quantum computers can achieve in the near term}

However, several of these problems seem to be far off from offering a benefit in the near term. And while it is interesting to consider problems which quantum computers will eventually be able to solve faster, there is still a need to motivate building these devices now. Hence, the fundamental question of this thesis is the following: What can a quantum computer do in the near future that a classical computer cannot?

As of 2019 there is arguably a very simple answer to this question: Random Circuit Sampling (RCS). This problem, first specified by Boixo et al.~\cite{boixo2018}, is to sample from the output of a random quantum circuit whose depth is linear in the number of qubits. This problem was proven to be classically hard under certain assumptions \cite{hangleiter2018, bouland2018} -- explained in more detail in Chapter \ref{chp:preliminary_bs}) -- but a quantum computer would be able to solve it by simply implementing the random circuit and making a measurement. And most excitingly, an instance of the problem has now been solved on a quantum computer developed by Google which we believe to be hard to solve classically in a reasonable amount of time \cite{arute2019}.

However, there are a number of limitations with this model. First, there are no known applications of RCS. Our interest in the problem is not because of its practical uses, but instead simply because we have strong evidence for the problem being classically hard. Second, Google's device is not especially good at performing RCS. The metric used to verify correctness, a variant of fidelity, is estimated for Google's device as roughly 0.1-0.2\%. Google defends this low fidelity by simply reducing their estimate of a classical computer's performance by a factor of 1,000, again emphasising classical hardness over usefulness. And third, there is still a lot of uncertainty and debate as to how quickly a classical computer can perform this problem. Google's paper estimated it would take a state of the art supercomputer 10,000 years \cite{arute2019}, whereas a recent preprint by IBM estimates the best current supercomputer could perform the same computation in roughly 2.5 days if used solely for this computation \cite{pednault2019}. Even further, a report by Morimae, Takeuchi and Tani suggests that classical computers could simulate Google's device even faster, as the fidelity is so low that even sampling from the uniform distribution could be sufficiently precise \cite{morimae2019google}.

It is with these limitations in mind that we broadly define what we mean when we refer to a near-term quantum speedup. Ideally, we want to find a problem that meets three requirements. The first is that this problem can be applied to real-world problems, rather than only being interesting because of its hardness. The second is that a quantum computer can be constructed in the next few years that can solve this problem in a reasonable amount of time. And the third is that this problem is infeasible to solve on a classical computer, or will at least take significantly longer than the time required for a quantum computer. We have purposefully left these requirements broad rather than giving specific details. This is in order to allow room for a variety of problems, which might satisfy some of the requirements more than others.

One direction that quickly comes to mind that fits these requirements well is the field of Noisy Intermediate-Scale Quantum (NISQ) devices \cite{preskill2018}. When Preskill coined the term at a Keynote Address, a variety of examples were given from quantum simulations to quantum semi-definite programming, as examples of practical problems with known quantum speedups that could be experimentally feasible in the near future. NISQ is often used to refer to two quantum systems in particular: The Variational Quantum Eigensolver (VQE) \cite{peruzzo2014}, which estimates eigenstates of some target Hamiltonian $H$ via a combination of quantum state preparation, Hamiltonian evolution and measurement; and the Quantum Adiabatic Optimisation Algorithm \cite{farhi2014}, which uses a combination of alternating unitary rotations and classical optimisation to solve various combinatorial optimisation problems. Both of these offer potential for meeting our requirements for a quantum speedup, and are strong directions of future research. However, we will not focus on these ideas for the content of this thesis, instead prioritising other research topics.

\section{Our contributions}

In this thesis, we make some progress towards finding this quantum speedup. We shall do this in two directions, laid out below.

\subsection{An application-focused perspective}

Our first direction is an application-focused, or ``top-down'', approach. This is through taking a problem that cannot be efficiently solved on a classical computer and seeing if a quantum computer can solve it faster.

The problem we focus on in particular is the Travelling Salesman Problem (TSP), where the aim is to find the fastest way of visiting every city on a map. This problem is well-known in Computer Science and Mathematics, and has been considered by academics for over 150 years \cite{schrijver2005}. As an NP-Hard problem, finding an efficient classical solution would solve the long-standing P vs NP problem, bringing with it a million dollar prize as well as many other significant consequences to the world around us \cite{cmipvsnp, aaronson2006}. Being NP-Hard also means that it is highly unlikely for an efficient quantum algorithm to be found, but it does not rule out the possibility that quantum computers can solve this problem faster than classical computers. Our first contribution is to demonstrate this positively, by proving polynomial speedups for the Travelling Salesman Problem in the special case where the graph is of bounded degree. This is achieved by showing how classical algorithms for solving the bounded-degree TSP \cite{eppstein2007, xiao2016degree3, xiao2016degree4} can be sped up using quantum algorithms \cite{montanaro2015, ronagh2019}.

This approach is directed at satisfying our first and third requirements, that the problem is applicable to the real-world and is difficult to solve classically. We shall not explicitly cover the second requirement, that a near-term quantum computer can solve this efficiently, as part of our contributions, but we will make some comments about other progress that has been made towards this requirement and what can be done to improve things further \cite{dunjko2018, ge2019}.

\subsection{An architecture-focused perspective}

Our second direction is an architecture-focused, or ``bottom-up'' approach. Here we will take a near-term architecture for a quantum computer and consider how much more powerful it could be than classical computation.

In particular, we shall consider Boson Sampling, a non-universal model of quantum computation consisting of sampling indistinguishable photons input into a random linear optical interferometer \cite{aaronson2010report, aaronson2011}. It was proven by Aaronson and Arkhipov that this architecture is classically infeasible to simulate for sufficiently many photons and a large enough interferometer, assuming certain conjectures related to matrix permanents and computational complexity hold. What was significant about Boson Sampling in particular was that this was a very simple model, with no error correction required, and simply used methods and components in linear optics which were already well-known and understood. This led to significant interest, including many experimental advances \cite{broome2013, spring2013, tillmann2013, crespi2013, carolan2015, wang2017, paesani2018, wang2019}, as well as architectural variants \cite{aaronson2013, lund2014, hamilton2017}.

However, there are practical issues with Boson Sampling experiments which were left unaddressed in the original result. In particular, how photon distinguishability and loss affect the classical complexity of Boson Sampling. These issues have been studied in recent years, with some exciting results \cite{garciapatron2017, renema2018, renema2018loss, oszmaniec2018, brod2019}. In this thesis, we make two contributions to this side. First, we describe a link between Boson Sampling and representation theory, via the Quantum Schur Transform \cite{bacon2004, harrow2005, bacon2007}. In doing so, we are able to model distinguishability and loss as different forms of decoherence in a specially-structured quantum circuit. Second, we take this model and apply it to a specific form of distinguishability and loss noted in \cite{renema2018, renema2018loss}. We then adapt the best asymptotic classical simulation algorithm for Boson Sampling \cite{clifford2017} to take advantage of distinguishability and loss, and in doing so produce an algorithm which can better simulate Boson Sampling in near-term experiments under these imperfections.

This direction is more aimed at satisfying our second and third requirements of a quantum speedup, in that Boson Sampling is a problem which can be easily done by a quantum computer, such as a Boson Sampling device, but is classically infeasible. This direction less satisfies the first option, in that there are not any known applications for the standard form of Boson Sampling. However, interferometers used for Boson Sampling have also been used for other applications, such as simulating the vibronic spectra of molecules \cite{sparrow2018}, and Boson Sampling variants have also found applications in both simulations and combinatorics \cite{huh2015, bradler2018, schuld2019}.

\subsection{Structure of this thesis}

The remainder of this thesis is laid out as follows. In Part \ref{prt:application}, we shall focus on the applications perspective. First, in Chapter \ref{chp:prelim-q-c} we explain the significance of NP-Hard problems, general classical algorithms for these problems and their corresponding quantum speedups. In Chapter \ref{chp:tsp} we give an explicit example of a speedup for an NP-Hard problem, by giving polynomial speedups for the Travelling Salesman Problem for bounded degree graphs. Next, in Part \ref{prt:architecture} we switch to the architecture perspective. We start in Chapter \ref{chp:preliminary_bs} by explaining the need for near-term quantum architectures, introduce the theory behind Boson Sampling and its imperfections, and explain representation theory of the Symmetric and Unitary groups. In Chapter \ref{chp:noisy_circuit}, we go into detail about the link between Boson Sampling and representation theory, and in doing so explain how distinguishability and loss can be modelled as decoherence in a quantum circuit of particular structure. We then in Chapter \ref{chp:classical_sim} use this model to develop a classical simulation algorithm for Boson Sampling under a particular model of these imperfections, and show that this algorithm will be a more effective simulator for near-term devices with 30-50 photons. We conclude with some final points and open questions in Chapter \ref{chp:conclusion}.

\subsection{Previous publications and collaborations}

Some of the contents of this thesis has been published previously, or completed in collaboration with others. All work in these chapters was led and written by myself. Permission for reuse of content, where applicable, has been acquired from the relevant publishers.

\begin{itemize}
\item Chapter \ref{chp:tsp} is joint work with Noah Linden and Ashley Montanaro, and has been published as ``Quantum Speedup of the Travelling Salesman Problem for bounded-degree graphs'', \href{https://link.aps.org/doi/10.1103/PhysRevA.95.032323}{\textit{Physical Review A} \textbf{95}, 032323 (2017)} [{\tt \href{https://arxiv.org/abs/1612.06203}{arXiv:1612.06203}}]\footnote{Note that this work was completed and published under my former name.}, copyright American Physics Society.

\item Chapter \ref{chp:noisy_circuit} is joint work with Peter S.\ Turner, and has been published as ``Quantum simulation of partially distinguishable boson sampling'', \href{https://link.aps.org/doi/10.1103/PhysRevA.97.062329}{\textit{Physical Review A} \textbf{97}, 062329 (2018)} [{\tt \href{https://arxiv.org/abs/1803.03657}{arXiv:1803.03657}}], copyright American Physics Society.

\item Chapter \ref{chp:classical_sim} is joint work with Ra\'{u}l Garc\'{i}a-Patr\'{o}n, Jelmer J. Renema and Peter S. Turner, and has been published as as ``Classically simulating near-term partially-distinguishable and lossy boson sampling'', \href{https://iopscience.iop.org/article/10.1088/2058-9565/ab5555}{\textit{Quantum Science and Technology} \textbf{5}, 015001 (2020)} [{\tt \href{https://arxiv.org/abs/1907.00022}{arXiv:1907.00022}}], copyright Institute of Physics.
\end{itemize}

\section{A brief note on notation}

Throughout this thesis we will make extensive use of Big-O (and related) notation to simplify the expression of runtimes and only focus on the most significant factors. We will not summarise this notation in detail, and instead point the reader to \cite{clrs} for further information, but we will note here a few variants also used. We shall use $O^*$ to suppress factors which are polynomial in $n$, the input size, as well as $\poly(n)$ to denote terms which are polynomial in $n$ and $\polylog(n)$ for terms which are polynomial in $\log n$.

We shall discuss a variety of complexity classes. P, NP and NP-Hard will be explained in more detail in Section \ref{sec:p-vs-np}, but informally: P is the class of problems which a classical computer can solve in polynomial time, NP the class of problems which a classical computer can verify a solution for in polynomial time, and NP-Hard the problems which are at least as hard as any problem in NP. We shall also mention BPP, the class of problems which can be solved probabilistically on a classical computer in polynomial time with probability greater than 2/3; BQP, the class of problems which can be solved on a quantum computer with probability greater than 2/3; and $\sharpp$, the class of problems around counting the number of solutions to a problem in NP. Further details on these any many other complexity classes can be found at the Complexity Zoo \cite{complexityzoo}.

We shall use $\unitary(m)$ to denote the Unitary group of degree $m$, which is the group of $m\times m$ unitary matrices, and $\symm_n$ to denote the Symmetric group of degree $n$, which is the group of permutations for $n$ objects. Irreducible representations, or irreps, of these groups over the vector space $(\mathbb{C}^m)^{\otimes n}$ are denoted $\lambda$, with $q_\lambda$ and $p_\lambda$ denoting basis states in these irreps for the Unitary and Symmetric groups, respectively.

The trace distance between two states is $\rho$ and $\sigma$ is defined as

\begin{equation}
\delta_{\trace}(\rho, \sigma) \colon= \frac{1}{2}\trace\left[\sqrt{(\rho-\sigma)^\dagger(\rho-\sigma)}\right].
\end{equation}

\noindent Note that this state is convex:

\begin{equation}
\delta_{\trace}(\rho, \sigma + \tau) \leq \delta_{\trace}(\rho, \sigma) + \delta_{\trace}(\rho, \tau).
\end{equation}

\noindent Similarly, the trace norm of a density matrix $\rho$ is defined as $\|\rho\| \colon= \trace\sqrt{\rho^\dagger\rho}$.

The generalised partial transposition criterion from \cite{chen2002} is a method for testing if a mixed state $\rho = \sum_{i,j}\rho_{i,j}\ket{i}\otimes\bra{j}$ is entangled. Denote a row transposition as $\rho^{T_{r}} = \sum_{i,j}\rho_{i,j}\bra{i}\otimes\bra{j}$ and column transposition as $\rho^{T_{c}} = \sum_{i,j}\rho_{i,j}\ket{i}\otimes\ket{j}$. The criterion states that $\rho$ is separable if for any partial transposition of rows or columns of subsystems $y \subset \{r_A,c_A,\dots,r_Z,c_Z\}$, $\|\rho^{T_y}\| \leq 1$.

The total variation distances between two probability distributions $P$ and $Q$ can be defined for a finite set of outcomes $\Omega$ as half the $L_1$ distance:

\begin{equation}
\Delta(P, Q) \colon= \frac{1}{2}\sum_{\omega \in \Omega}|P(\omega) - Q(\omega)|.
\end{equation}
