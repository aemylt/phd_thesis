\chapter{Conclusion}
\label{chp:conclusion}

It truly is astounding to see what progress has been made in quantum computing. From the 1980s, when quantum computers were a vague concept with no theoretical benefits beyond possibly simulation \cite{feynman1982}, to the 1990s when the first theoretical speedups were discovered \cite{shor1994, grover96}, to today where demonstration devices are now readily available and showcasing the potential that quantum computers hold \cite{rigetti, ibm}. All of this culminating in Google's astonishing result: a quantum computer solving a problem significantly faster than what even the best supercomputers can accomplish \cite{arute2019}. This is a tremendous way to mark the start of a new decade in quantum computing.

And with this new decade, new goals must be sets. Now that we have quantum computers solving a problem faster than what is classically possible, we need to show that quantum computers can do so for a problem that is beneficial to us. And just as importantly, we need to show that quantum computers can solve this problem well.

In this thesis, we have strived to close the gap between what problems quantum computers can solve and the limitations of their architecture. In Part \ref{prt:application}, we have given a new example problem which quantum computers can offer a speedup for over the best classical algorithms: The Travelling Salesman Problem. As an NP-Hard problem with many applications, this demonstrates how quantum computers can offer speedups for even some of the most challenging problems we encounter in Computer Science and Mathematics. In Part \ref{prt:architecture}, we assess how photon distinguishability and loss affect the near-term quantum advantage architecture of Boson Sampling. In doing so, we devise new methods for mathematically modelling these imperfections via the first quantisation, and from there adapt the classical Clifford \& Clifford algorithm \cite{clifford2017} to take advantage of these issues. This emphasises how much these imperfections reduce any benefit we are likely to see, and how much effort we must do to overcome these experimental challenges in the near future.

\section{Open Questions}
\label{sec:oq}

But alas, the gap between these directions still exists, and there is yet more work to be done. We shall conclude this thesis with some open questions, such that an inspired reader might choose to pursue these routes even deeper. As we have done so throughout this thesis, we shall consider the two approaches separately.

\subsection{Applications direction}

Several open directions have already been mentioned in Section \ref{sec:tsp-conclusion}. Of particular note is the question of what other classical algorithms for the Travelling Salesman Problem can be sped up by a quantum computer. Promising directions include cut-and-count \cite{bjorklund14,bodlaender15,cygan11}, which use a combination of Monte Carlo algorithms and dynamic programming, and branch-and-bound \& branch-and-cut \cite{little1963, padberg1991, applegate2006}, for which quantum speedups already exist but most complexity understanding is empirical rather than analytical.

The fact that such speedups exist for the Travelling Salesman Problem suggests that polynomial speedups also exist for other NP-Hard problems as well. Already several other problems have seen speedups: Campbell, Khurana and Montanaro \cite{campbell2019} showed how the backtracking framework can also be applied to Boolean Satisfiability and Graph Colouring, and Montanaro \cite{montanaro2019} used the quantum speedup for branch-and-bound algorithms to find the exact ground state of Sherrington-Kirkpatrick Hamiltonians. Due to the same families of algorithms, such as dynamic programming or backtracking, being used to solve many different NP-Hard problems, it is likely that in the future we shall see polynomial speedups for many more.

But of course, this still does not address the main stumbling block when moving from these eventual speedups to a near-term speedup: That the quantum algorithm needs to run on near-term quantum computers. As noted in Campbell, Khurana and Montanaro \cite{campbell2019} and mentioned in Section \ref{sec:error-correction}, this already rules out many of these algorithms, for which the significant dependence on error correction and fault tolerance, and the classical computational overhead the comes with it, reduces any quantum speedup in practice to nought. To overcome this, we need to see these algorithms adapted to architectures which might be realisable in the near future.

We already hint at this open direction in Section \ref{sec:error-correction}, when discussing the recent work on algorithms which use classical processing to break the problem down until it is of a size that a quantum algorithm with a small number of qubits can be used \cite{dunjko2018, ge2019}. It is likely that other quantum algorithms for these problems can also be adapted to a hybrid quantum-classical framework of this form. One that we think is particularly promising is the approach of Ambainis et al.~\cite{ambainis2018}, where a polynomial speedup for dynamic programming is obtained by using a classical computation to preprocess simple spaces of the problem, followed by using Quantum Minimum Finding to search over the larger spaces. It might be possible to adapt this architecture to quantum computers with constrained amounts of memory, by more careful analysis of the quantum algorithm's memory usage and new consideration of how to partition the quantum and classical aspects of the algorithm. It is worth noting that this approach still depends on universal fault-tolerant quantum computation, but the hope is that the smaller number of logical qubits required would lead to a more feasible speedup.

There are also other near-term architectures that can be considered. Of particular note is the Quantum Adiabatic Optimisation Algorithm. When originally proposed by Farhi et al.~\cite{farhi2014}, it was used as a quantum algorithm for finding an approximation to the Max-Cut problem, which is known to be NP-Hard \cite{karp1972}. As a model of universal quantum computation \cite{lloyd2018, morales2019}, it is likely that other applications for QAOA to NP-Hard problems will also be found in the future, though it is less clear how promising a speedup QAOA will offer. Other intermediate-scale, or NISQ, devices also show potential in different applications, such as the Variational Quantum Eigensolver for simulating physical systems \cite{peruzzo2014, preskill2018}.

\subsection{Architecture direction}

There are many directions we can go when looking at near-term quantum architectures. As mentioned at the end of Chapters \ref{chp:noisy_circuit} and \ref{chp:classical_sim}, the most natural direction for continuing this work is to better understand the computational complexity of Boson Sampling under these imperfections. We have already discussed several examples of classical simulation approaches, such as \cite{renema2018, renema2018loss, garciapatron2017, oszmaniec2018, brod2019}, and have hinted at how one might prove hardness, via the computational complexity of immanants \cite{hartmann1985, barvinok1990, burgisser2000, burgisser2000immanants, mertens2013}. Better understanding of how the Quantum Schur Transform acts on certain states would also assist in this goal.

But Boson Sampling is only one of many photonic quantum computing architectures. Another interesting question is whether or not these same imperfections can be applied to variants of Boson Sampling. Many of these simulation algorithms, including our own described in Chapter \ref{chp:classical_sim}, can be naturally extended to Scattershot Boson Sampling, by simply choosing which modes our photons start in from the $\binom{m}{n}$ uniform distribution at the start. But what about Gaussian Boson Sampling? Already there have been some promising results by expanding some of the classical algorithms for vanilla Boson Sampling \cite{qi2019, quesada2019, wu2019, renema2019}, and it would be exciting to see the same for other classical algorithms. It would also be interesting to see if these algorithms could be extended to universal quantum photonic architectures, through simulating postselected, adaptive or measurement-based schemes \cite{knill2001, gimenosegovia2015}.

More broadly, we still need to work hard to find applications for Boson Sampling and its variants. As has already been mentioned, a number of promising directions exist, particularly for more general linear optics \cite{sparrow2018} and Gaussian Boson Sampling \cite{huh2015, bradler2018, schuld2019}. It will be interesting to see what other applications exist, and especially if these applications extend to other schemes.

And beyond linear optics, we need to look at the many quantum advantage proposals that have been proposed over the last several years, to see what extent we can classically simulate them as well as what applications they might have. Boson Sampling has received a lot of attention on this end, as already shown. Less is currently known about the extent to which IQP circuits and Random Circuit Sampling can be classically simulated. But there are some promising results in these directions. For IQP circuits, Bremner, Montanaro and Shepherd showed how polarisation error can lead to an efficient classical simulation, as well as how simple forms of error correction can mitigate these issues \cite{bremner2017}. As for Random Circuit Sampling, Pednault et al.~\cite{pednault2019} give an estimate for what is likely to be the largest classical simulation we can achieve, and Morimae, Takeuchi and Tani \cite{morimae2019google} suggest a classical simulator which takes advantage of the poor fidelity. Another relevant direction, considered by Bravyi et al.~\cite{bravyi2019}, looks at how much we can classically simulate random quantum circuits when there are only a small number of non-Clifford gates\footnote{Note that quantum circuits composed entirely of Clifford gates are efficiently classically simulable, and the inclusion of non-Clifford gates such as the $T$ or Toffoli gates is sufficient for universal quantum computation.}, showing that circuits with 40--50 qubits and over 60 non-Clifford gates can be simulated on a standard computer.

\section{The future}

So what lies ahead for the world of quantum computing then? It is always hard to predict what will happen, but the current state of quantum computers offer a lot of promise and potential. Google's paper has proven that potentially relevant and significant quantum devices could be realisable within our lifetime, if not already here. But in order to truly demonstrate that these devices are worth the hype and effort behind them, now is more important than ever that we push for improving the architecture to as good a quality as possible, as well as refining the problems we are proposing for them to make them more physically realisable.

Eventually, we hope that these two paths will intersect, and we will have a useful application for a quantum computer. Then, dear reader, we will have found our quantum speedup.
