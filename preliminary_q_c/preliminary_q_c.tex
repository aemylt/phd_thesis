\chapter{Preliminary Material: Quantum Algorithms for NP-Hard Problems}
\label{chp:prelim-q-c}

An intuitive starting point for finding a problem which quantum computers can demonstrably outperform classical machines at would be to find problems which are classically intractable. There are many problems to choose from, with perhaps the most famous being those which are described as NP-Hard. Many problems with practical applications fit into this family, and although these problems have not been formally proven to be hard for classical computers, the consequences of them being easy to solve has led to the general consensus that these problems are intractable\footnote{For some informal arguments as to why these problems \textit{should} be classically hard, see \cite{aaronson2006}}.

The reasons above suggest that these problems might be suitable for a demonstrable quantum advantage. However, the downside is that these problems are generally believed to be too difficult for quantum computers to solve efficiently as well! So it seems unlikely that any exponential speedup is achievable, but what about a polynomial speedup? Although less impressive, a polynomial improvement can still be significant in practice.

The next two chapters will focus on this question. In this chapter we shall survey the world of NP-Hard problems. In particular we will look at classical and quantum algorithms for solving these problems in general. In Chapter \ref{chp:tsp}, we shall demonstrate how these quantum speedups can be applied to special instances of the Travelling Salesman Problem, an especially famous NP-Hard problem.

\section{P vs NP and NP-Hardness}

To understand why NP-Hard problems are a good starting point in the hunt for a quantum speedup, we first need to understand why they are so important in the world of complexity theory. This is because they provide a potential solution to one of the famous problems in computer science and mathematics: The P vs NP problem.

Informally, P and NP are sets of decision problems, or languages. P, short for Polynomial Time, is the set of decision problems which can be \textit{solved} in polynomial time on a classical computer. This means that a language $L \in \p$ if there is a polynomial-time classical algorithm such that for all words $w \in L$ our algorithm accepts $w$, and for all words $w \notin L$ our algorithm rejects $w$. NP in comparison, is short for Non-deterministic Polynomial Time, and can be thought of as the set of decision problems which can be \textit{verified} in polynomial time on a classical computer. This means that there is a polynomial-time classical algorithm such that for all words $w \in L$ there exists a certificate $c$ such that our algorithm accepts $(w,c)$, and for all words $w \notin L$ our algorithm rejects $(w,c)$ for all certificates $c$.

The P vs NP problem is the question of whether or not these two sets are equal. Intuitively it is easy to see that $\p \subseteq \np$: If a polynomial time algorithm exists for solving a decision problem then we can ignore the certificate and simply run the solver to verify the problem in polynomial time. So the main question is whether or not the opposite holds: Does $\p \supseteq \np$? It might seem intuitive that there are problems which are easy to verify but not easy to solve, but to this day there is no proof that this is the case. Indeed, solving this problem is now worthy of a million dollars courtesy of the Clay Mathematics Institute \cite{cmipvsnp}.

So where do NP-Hard problems come in? Well, a potentially good direction for solving the P vs NP problem is to focus on the very hardest problems in NP. NP-Hardness is a way of classifying these problems. More strictly, a language $L$ is NP-Hard if for any language $L' \in \np$, there exists a polynomial time classical algorithm which takes any word $w'\in L'$ to a word $w \in L$ and any word $w'\notin L'$ to a word $w \notin L$. Such an algorithm is known as a polynomial time \textit{reduction}.

These reductions mean that any problem in NP cannot require more time to solve than an NP-Hard problem. To see this, suppose we had a algorithm for solving an NP-Hard problem $L$ in time $T(n) \in \Omega(\poly(n))$, where $n$ is the size of our input. Then we immediately have an algorithm for solving any problem $L'$ in NP in time $T(n)+\poly(n)$: Given $w'$, we run our polynomial time reduction to create a word $w$, and then run our $T(n)$-time algorithm for deciding if $w \in L$.

This shows why NP-Hard problems are such a strong motivation for the P vs NP problem. Finding a polynomial time algorithm for a single NP-Hard problem is sufficient for proving that $\p = \np$. On the other hand, many NP-Hard problems have also been proven to be in NP, otherwise known as NP-Complete problems. If one of these NP-Complete problems can be proven to not be solvable in polynomial time, then we have proven that $\p \neq \np$. However, both directions of work have proven to be highly non-trivial to solve.

\subsection{Examples of NP-Hard Problems}

There are many examples of problems which are NP-Hard. For this summary, we shall give three well-known problems which are often discussed in the theoretical computer science literature and which the algorithms discussed later in this chapter are applicable to. For more examples of NP-Hard problems, we point the reader towards \cite{karp1972, garey1979}, which also includes reductions for these and many other problems.

\begin{problem}[Boolean Satisfiability (SAT)]
Let $B$ be a Boolean formula with variables $x_1,\dots,x_n$. Is there an assignment of $\mathbf{x}$ such that $B(\mathbf{x}) = \true$?
\end{problem}

Boolean Satisfiability was the first example of a problem proven to be NP-Complete, in what is now known as the Cook-Levin Theorem \cite{cook1971}. This is also true for different restrictions of Satisfiability, such as when the Boolean formulae are written in Conjunctive Normal Form with at least three terms in each clause \cite{karp1972}.

The proof that Satisfiability is NP-Complete is non-trivial, and is proven by showing that any polynomial-time non-deterministic algorithm running on some input can be reduced to a Boolean formula. Proving subsequent problems to be NP-Hard is easier however, as the proof just needs to show that a single NP-Hard problem can be reduced to this new problem. This paved the way for many other problems to be proven NP-Hard.

\begin{problem}[Integer Linear Programming]
Given $A \in \mathbb{Z}^{m\times n}$, $c \in \mathbb{Z}^n$ and $b \in \mathbb{Z}^m$, find $x\in\mathbb{Z}_\geq^n$ which maximises $c^Tx$ subject to $Ax \leq b$.
\end{problem}

It is worth noting that Integer Linear Programming is considered NP-Hard but not NP-Complete, as it is phrased as an optimisation problem rather than a decision problem. The equivalent decision problem -- does there exist an $x \in \mathbb{Z}_\geq^n$ that satisfies $Ax \leq b$ -- is NP-Complete, with the version where $x\in\{0,1\}^n$ being one of Karp's 21 NP-Complete problems \cite{karp1972}.

\begin{problem}[The Travelling Salesman (TSP)]
Let $G = (V,E)$ be a weighted graph with $n$ vertices. Find an minimum-weight cycle which visits every vertex exactly once.
\end{problem}

A solution to the Travelling Salesman Problem is by definition a Hamiltonian cycle for a graph $G$. Determining whether or not a Hamiltonian cycle exists in a graph was also proven to be NP-Complete by Karp in \cite{karp1972}.

Like Integer Linear Programming, the Travelling Salesman Problem is an optimisation problem rather than a decision problem. The decision variant -- does $G$ contains a Hamiltonian cycle of length at most $\ell$ -- is also NP-Complete, and can be seen through the same reasoning as the paragraph above. It is also worth noting that if an algorithm exists for the decision version of the problem, an algorithm for the optimisation problem can also be found with polynomial overhead, by calling the algorithm with varying bounds $\ell$ chosen in a binary search fashion.

\section{Classical Algorithms for NP-Hard Problems}

We shall now move on to exploring how we can classically solve these NP-Hard problems. We shall focus on exact algorithms, which are required to run in polynomial time classically. It is worth noting that there are classical algorithms for approximating many of these problems in polynomial time; Christofides' Algorithm, for example, provides a Hamiltonian cycle whose length is at most 3/2 of the length of the optimal Hamiltonian cycle in polynomial time \cite{christofides1976}. See \cite{garey1979} for a more thorough review of classical algorithms, including approximation algorithms.

In some cases the best performance we are able to asymptotically achieve is a brute force evaluation. With SAT for example, there are $2^n$ possible assignments of $n$ variables, and evaluating a single assignment takes polynomial time. Assuming the Strong Exponential Time Hypothesis holds, which in turn would imply that $\p \neq \np$, any classical algorithm must solve the SAT problem in $\Omega(2^n)$ time. But even when asymptotically brute force approaches are as good as any, in practice there might be many instances which do not require a full evaluation of all possible outcomes. This has led to significant interest in developing faster algorithms for non-worst-case Boolean formulae, with numerous surveys and competitions assessing these approaches \cite{sohanghpurwala2017, sat18}.

\subsection{Dynamic Programming}

Dynamic programming is a method for solving optimisation problems in a recursive fashion. The technique works by computing smaller instances of the problem and storing the result in memory in order to prevent needing to recompute the instance later.

The canonical example of dynamic programming is for computing Fibonacci numbers. A na\"{i}ve algorithm would compute the $n$-th Fibonacci numbers by recursively computing the previous two and adding the result together:

$$F(n) = F(n-1) + F(n-2)$$

This algorithm is inefficient as the same values will be computed many times over. For example, $F(n-2)$ will be computed twice, being called by both $F(n)$ and $F(n-1)$. A more efficient method is a ``ground-up'' approach: first set $F(1)=1$ and $F(2)=1$ in memory, and then for each $i \in \{3,\dots,n\}$, compute $F(i)$ using the previously computed answers and save the result in memory.

For NP-Hard problems, dynamic programming has offered a number of results. For many years, the algorithm with the best proven worst-case bounds for the Travelling Salesman Problem was the Held-Karp algorithm~\cite{held1962}, which runs in $O(n^22^n)$ time and uses $O(n2^n)$ space. This algorithm uses the fact that for any shortest path, any subpath visiting a subset of vertices on that path must be the shortest path for visiting those vertices. Held and Karp used this to solve the TSP by computing the length of the optimal route for starting at vertex $1$, visiting every vertex in a set $S \subseteq V$ and finishing at a vertex $l \in S$. Denoting the length of this optimal route $D(S, l)$, they showed that this distance could be computed as
%
\begin{equation}
D(S, l) = \begin{cases} c_{1l} & \text{if } S = \{l\}\\
          \min_{m \in S \setminus \{l\}}\left[D(S \setminus \{l\}, m) + c_{ml}\right] & \text{otherwise.}
  \end{cases}
\end{equation}
%
Solving this relation recursively for $S=V$ would result in iterating over all $O((n-1)!)$ Hamiltonian cycles again, but Held and Karp showed that the relation could be solved in $O(n^22^n)$ time using dynamic programming. Bj{\"o}rklund et al.\ \cite{bjorklund2008} developed on this result, showing that modifications to the Held-Karp algorithm could yield a runtime of
%
\begin{equation}
O((2^{k + 1} - 2k - 2)^{n/(k + 1)}\poly(n)),
\end{equation}

\noindent where $k$ is the largest degree of any vertex in the graph; this bound is strictly less than $O(2^n\poly(n))$ for all fixed $k$.

\subsection{Backtracking}

Backtracking is a form of recursive algorithm designed for solving Constraint Satisfaction Problems (CSPs). These are problems where the input is a set of variables $x_1,\dots,x_n$ and the aim is to find an assignment for these variables satisfying constraints $c_1,\dots,c_n$.

Backtracking works by taking a set of already assigned variables and simplifying the constraints accordingly. We then use a predicate $P$ to check if the constraints are already (un-)satisfiable under the current assignments. If so, then we are done. Otherwise, we choose an unassigned variable according to some heuristic $h$, and then recursively call the algorithm on all possible assignments.

\begin{algorithm}
\Fn{\FnBTSAT{$B$, $\tilde{\mathbf{x}}$}}{
\KwIn{A boolean formula $B$, a partial assignment of variables $\tilde{\mathbf{x}}$}
\KwOut{A satisfying assignment or $\emptyset$ if no such assignment exists}
\uIf{$B(\tilde{\mathbf{x}}) = \true$}{
Return $\tilde{\mathbf{x}}$\;
}\uElseIf{$B(\tilde{\mathbf{x}}) = \false$}{
Return $\emptyset$\;
}\Else{
Choose unassigned variable $x_i$ uniformly at random\;
\eIf{\FnBTSAT{$B$, $\tilde{\mathbf{x}}(x_i=\true)$}}{
Return \FnBTSAT{$B$, $\tilde{\mathbf{x}}(x_i=\true)$};
}{
Return \FnBTSAT{$B$, $\tilde{\mathbf{x}}(x_i=\false)$};
}
}
}
\caption{\label{alg:backtracksat} A backtracking algorithm for SAT.}
\end{algorithm}

Algorithm \ref{alg:backtracksat} gives an example of a backtracking algorithm for Boolean Satisfiability. Asymptotically, this algorithm will require $O(2^n\poly(n))$ time in the worst case, where every possible assignment needs to be tested. However, in practice we might not need to evaluate large portions of the tree, due to finding partial assignments which already succeed or fail to satisfy the Boolean formula.

\subsubsection{Branch and Bound}

Branch and Bound is a development of backtracking specifically looking at optimisation problems. Rather than proceeding recursively like in Backtracking, Branch and Bound uses a bound function to decide the order in which to evaluate potential solutions, as well as a branch function to decide on how to reduce the solution space.

Branch and Bound is a standard approach for Integer Linear Programming. An upper bound can be computed by removing the constraint that the solution vector $x$ needs to be an integer, relaxing the problem to that of a Linear Programming problem which can be solved in polynomial time via, for example, \cite{cohen2019}. If $x$ is an integer solution, no solution exists, or the upper bound is worse then our current best solution, we stop exploring that potential solution. Otherwise we branch by choosing an $s = x_i$ in the solution vector which is not an integer, and create two new reduced problems by adding either the constraint that $x_i \leq \lfloor s \rfloor$ or that $x_i \geq \lceil s \rceil$. We can also improve the algorithm even further by adding constraints in the form of cutting planes; such a technique is commonly referred to as Branch and Cut.

Both Branch and Bound \& Branch and Cut algorithms have also been developed for the Travelling Salesman Problem \cite{little1963, padberg1991}. Asymptotic analysis of these algorithms are hard to come by, but these approaches have been found to be the best performing in practice. Indeed, a Branch and Cut algorithm is responsible for some of the largest solved instances of the Travelling Salesman Problem to date, finding the optimal tour of 85,900 cities in 2005/2006 \cite{applegate2006}, and later finding the optimal tours of 49,687 UK pubs and 109,399 stars \cite{tspuk49687}.

\section{Quantum Speedups}

We shall now summarise some of the research in quantum speedups of the classical algorithms already described in this chapter. For a more broad summary of quantum algorithms, we direct the reader to \cite{montanaro2016}.

\subsection{Quantum Search}

The most immediate way in which one might try to solve NP-Hard problems is with quantum search, originally developed by Grover \cite{grover96}. Given an oracle for checking if a solution is correct, Grover search works by querying this oracle over a superposition of different solutions. If there is a total of $n$ solutions, then it is possible to show that Grover search will find a correct solution after $O(\sqrt{n})$ queries, whereas an unstructured classical search would require $O(n)$ queries in the worst case.

Grover search has proven beneficial for a number of quantum algorithms. For instance, it has been shown that if there are $m$ correct solutions then Grover search will succeed after $O(\sqrt{n/m})$ queries. There is also a more general algorithm called amplitude amplification, where if the probability of a quantum algorithm outputting a correct solution is $a$, then a correct solution can be found after running the quantum algorithm $O(1/\sqrt{a})$ times \cite{brassard2002}. It is also possible to find the minimum solution in $O(\sqrt{n})$ queries, by choosing a threshold and periodically updating it as the algorithm runs \cite{durr1996}.

This seems like a reasonable starting place for trying to solve NP-Hard problems. Many of these problems, including the ones given above, are related to either proving a solution exists, or finding a minimum solution. Quantum search algorithms then provide a quadratic speedup over searching for all possible solutions. For example with SAT, our search space is all $2^n$ possible assignments of $n$ variables, meaning that Grover search would find a satisfying assignment or prove that one does not exist in $O(2^{n/2})$ queries. Assuming the Strong Exponential Time Hypothesis holds, this guarantees a quadratic speedup in worst-case performance.

However, a quadratic speedup over \textit{any} classical algorithm is not necessarily guaranteed. Take the Travelling Salesman Problem for example: the number of possible solutions is on the order of the number of permutations of vertices, of which there are $n!$, meaning that the quantum minimum finding algorithm would find a shortest Hamiltonian cycle in $O(\sqrt{n!})$ queries. But there are already classical algorithms which are significantly faster. The Held-Karp algorithm, for example, which uses $O(n^22^n)$ queries. As a result, we need to consider other speedups to gain an improvement over these better classical algorithms.

\subsection{Dynamic Programming}

Until recently, it was not known whether or not quantum algorithms would be able to speed up dynamic programming algorithms. This is because of the way in which dynamic programming typically records the solutions to all possible subproblems in memory. Adapting these methods for quantum computers is nontrivial as a result.

The first improvement on dynamic programming algorithms was given by Ambainis et al.\ \cite{ambainis2018}, which considered the Path in the Hypercube problem. This problem is based around the $2^n$ Boolean hypercube, which is a graph where each vertex represents an $n$-bit string and two vertices are adjacent if their Hamming distance is 1. The aim is to find a path from $0^n$ to $1^n$ which only uses some subgraph of the Boolean hypercube.

Ambainis et al.\ provide a quantum algorithm for solving this problem in $O^*(1.817^n)$ time. The fundamental idea of this technique is to use classical dynamic programming to solve subproblems close to $0^n$ and $1^n$, before using quantum search as described above to find a path between the two subproblems. Allowing the quantum algorithm to solve larger subproblems leads to a faster runtime, but improvements over time start becoming too small to be significant. Ambainis et al.\ reach the above runtime of $O^*(1.817^n)$ when the quantum algorithm starts running at depth $6$. When the subgraph has at most $\mu^n$ vertices for $\mu \geq 1.735$, this leads to speedups in other dynamic programming algorithms too.

Ambainis et al.\ then develop a related algorithm for solving the Travelling Salesman Problem, as well as similar NP-Hard problems. This algorithm works by using classical computation to compute all ways of partitioning the set of vertices into two sets of size $k$ and $n-k$ for some $k$ and solving the TSP in these subspaces. The quantum subroutine then uses Grover search over all these subproblem solutions to find pairings that give an optimal solution to the TSP. Ambainis et al.\ found that the optimal runtime is when $k=n\alpha/4$ for $\alpha\approx0.055362$, which gives a runtime of $O^*(1.728^n)$. This algorithm can be generalised to other dynamic programming algorithms as well, by generalising from the set of vertices to more general sets with some cost function.

This algorithm gave a speedup over classical algorithms such as the Held-Karp algorithm, which runs in $O(n^22^n)$ time \cite{held1962}. However, this is not a quadratic speedup, which is what we would ideally want. It is also worth noting that the classical algorithm of \cite{bjorklund14}, which runs in $O*(1.657^nL)$ where $L$ is the largest edge weight is more efficient assuming that the edge weights are at most a constant\footnote{In fact, the algorithm of \cite{bjorklund14} also performs when the edge weights are polylogarithmic in $n$, as the quantum algorithm of \cite{ambainis2018} also requires an $O(\log L)$ overhead for various edge-weight arithmetic, as do other algorithms such as Held-Karp \cite{held1962}.}.

A quadratic speedup for dynamic programming was later proven by Ronagh \cite{ronagh2019}. This works by a technique called the Multiplicative Weights Update Method. In this method, there are $n$ experts, each of which advise the algorithm on the next step to take, after which the algorithm updates its weighting of each expert. Initially these experts are evenly weighted, and over time some experts become more favoured over others, until the algorithm halts after $T$ iterations for some $T$. The updating of weights after each iteration is based on computation of some cost vector. A quadratic speedup is achieved via quantum minimum finding \cite{durr1996}. This can be applied to dynamic programming via a dual formulation of dynamic programming methods. Applying all of this leads to a quantum algorithm for the Travelling Salesman Problem in $O^*(L^42^{n/2})$.

\subsection{Backtracking}

\subsubsection{Branch and Bound}

\section{Estimated speedups in practice}